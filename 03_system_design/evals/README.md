# Evaluating Agentic AI Systems 

This page provides an overview of evaluation methods in agentic AI systems, including the latest frameworks, benchmarks, and tools available in 2025.

Evaluating AI agents is essential for building robust and effective agentic applications. Due to their complex architecture, it is important to assess different aspects of agent performance using targeted metrics and tools.

**Evaluation Landscape in 2025:**
- Evaluation has matured into a standardized practice with well-defined testing frameworks
- Multiple specialized benchmarks available for different agent capabilities (web agents, OS agents, tool-using agents)
- New evaluation tools and platforms have emerged for production-scale agent testing
- Focus on real-world scenarios and non-deterministic testing approaches

This guide outlines key categories and metrics to consider when evaluating AI agents. Not all metrics will be relevant for every use caseâ€”choose those that best align with your project goals.

## Contents: 
  - [Gen AI Systems Evals](./gen-ai-evals.md): Evaluation methods for non-agentic GenAI systems
  - [Agentic AI Systems Evals](./agentic-ai-evals.md): Comprehensive evaluation framework for agentic AI systems
  - [AI product evals](./ai-product-evals.md): Product-level evaluation metrics and practices

## Key Evaluation Tools & Frameworks (2025)

### Evaluation Platforms
- **Maxim AI**: End-to-end evaluation platform for AI agents
- **Langfuse**: Open-source observability and evaluation for LLM applications
- **Comet Opik**: AI agent evaluation and monitoring
- **Arize**: ML observability platform with agent evaluation capabilities
- **Evidently AI**: AI agent benchmarks and testing frameworks

### Popular Benchmarks
- **Web Agent Benchmarks**: Evaluate agents' ability to navigate and interact with web interfaces
- **OS Agent Benchmarks**: Test agents' capabilities in operating system environments
- **Tool-Using Agent Benchmarks**: Assess agents' proficiency with external tools and APIs
- **Multi-Agent Benchmarks**: Evaluate collaborative agent systems

### Testing Frameworks
- Specialized frameworks for non-deterministic AI agents
- Real-world scenario testing approaches
- Performance metrics and reliability assessment tools
